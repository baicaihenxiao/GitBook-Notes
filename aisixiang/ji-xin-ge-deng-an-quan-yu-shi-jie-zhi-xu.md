# 基辛格等：安全与世界秩序

[https://www.aisixiang.com/data/143416.html](https://www.aisixiang.com/data/143416.html)



如果一个国家面对的对手已经训练人工智能来驾驶飞机、独立瞄准并决定开火，这将如何影响战术、战略或诉诸升级战争规模（甚至是核战）的**意愿**？

人工智能开辟了信息空间能力，包括虚假信息领域。生成式人工智能可以创造大量似是而非的虚假信息，包括伪造的人物、图片、视频和演讲等，以此为**信息战和心理战**推波助澜。理论上，人工智能可以合成看似真实的冲突照片和视频，让公众人物“发表”他们其实从未说过的言论，并将其以最有效地方式广泛转发给目标群体，迎合人们的偏见和期望。如果一个国家领导人的合成形象被对手操纵，以制造不和或发布误导性指令，公众（甚至其他政府和官员）能够及时识破骗局吗？



然而，最深层的挑战可能是哲学层面上的。如果**战略的分析运作无法再为人类理性所理解**，则其过程、范围和最终意义将变得不再透明。如果决策者认为，在揭示现实的最深层模式、了解（可能拥有自己的人工智能的）对手的能力和意图并及时做出应对的过程中，人工智能的辅助已必不可少，那么，将关键决策权下放给机器可能成为必然趋势。各个社会可能会就“**哪些决策权可以下放**”、“何种风险和后果是可以接受的”等问题，给出各不相同的答案。主要大国应当未雨绸缪，提前就该演变的战略、学说和道德影响展开对话，否则将导致不可逆的影响。国际社会必须做出努力，以限制这些风险。







进入专题： [世界秩序](https://www.aisixiang.com/data/search?searchfield=keywords\&keywords=%e4%b8%96%e7%95%8c%e7%a7%a9%e5%ba%8f) &#x20;

● **亨利·基辛格**   **埃里克·施密特**   **丹尼尔·胡腾洛赫尔** &#x20;

&#x20;

近日，美国前国务卿亨利·基辛格和谷歌前首席执行官埃里克·施密特、麻省理工学院苏世民学院院长丹尼尔·胡腾洛赫尔共同推出新著《人工智能时代与人类未来》。在本书中，基辛格指出，新兴的人工智能技术极有可能颠覆性地重塑全球安全格局，并有可能加剧中美等大国之间的竞争与对立。他呼吁技术领先国家对此加以重视，未雨绸缪，防患未然，就人工智能军事用途的限制积极展开对话。本文为本书第五章《安全与世界秩序》，转自观察者。

**数字时代的冲突**

纵观历史，一个国家的政治影响力往往与其军事力量和战略能力大致匹配。所谓战略能力，也就是说，通过这种能力，即使仅仅施加隐性威胁，也会对其他社会造成破坏。然而，这种基于力量权衡的均势不是静态的，不能自我维持。相反，均势的维持首先依赖于各方就这一力量的构成要素及其使用的合法界限达成共识；其次，它需要体系内所有成员——尤其是对手——就各个国家的相对能力、意图和侵略的后果进行一致的评估；最后，它需要一个实际的、公认的平衡。当体系中某一方的力量不成比例地增长、打破平衡时，体系将组织对抗力量或适应新的现实，以设法做出调整。当均势的权衡变得不确定，或当各国对各自相对实力的权衡结果完全不同时，由误算引发冲突的风险就会达到最高。

在当今时代，所谓“网络武器”的出现使上述权衡更具抽象性。网络武器涉及军用和民用两个领域，因此其作为武器的地位是模糊的。在某些情况下，网络武器之所以能行使和增强军事力量效用，主要是因为其使用者未披露其存在或未承认其全部能力。传统上，冲突各方都不难认识到发生了交战，或者认识到交战各方是谁，会计算对方的战力，并评估其部署武器的速度。可是，这些在传统战场上颠扑不破的道理却不能直接套用到网络领域。

常规武器和核武器存在于物理空间中，其部署可以被察觉，其能力可以被（至少粗略地）计算。相比之下，网络武器的效用很大程度上源于其不透明性；若被公之于众，其威力就会减损。网络武器利用不为人知的软件漏洞，在未经授权用户许可或知情的情况下，侵入网络或系统。在“分布式拒绝服务”（DDoS）攻击（如对通信系统的攻击）的突发事件中，攻击者可能会使用大量看似有效的信息请求来压垮系统，使之无法正常使用。在这种情况下，攻击的真实来源可能被掩盖，使人（至少在当时）难以或无法确定攻击者。即使是最著名的网络工业破坏事件之一—破坏了伊朗核项目中制造控制计算机的震网（Stuxnet）病毒，也从未被任何政府正式承认。

常规武器和核武器可以相对精确地瞄准目标，根据道德和法律的要求，其瞄准对象只能是军事力量和军事设施。而网络武器可以广泛影响计算和通信系统，往往可对民用系统造成有力打击。其他行为体也可基于其他目的，对网络武器进行吸纳、修改和重新部署。某种程度上，网络武器与生化武器类似，其影响能够以非预期的、未知的方式传播，往往不仅影响战场上的特定目标，且波及大范围的人类社会。

因此，网络军备控制难以被概念化，从而进一步推行。核军备控制的谈判人员可以公开披露或描述一类核弹头，不必否认其功能。而（目前尚不存在的）网络军备控制谈判人员则需要面对以下悖论：一旦对网络武器的威力进行讨论，可能就会导致这种威力的丧失（允许对手修补漏洞）或扩散（对手得以复制代码或侵入方法）。

关键网络术语和概念的模糊性使这些挑战变得更加复杂。在不同的背景下，不同的观察者将形式各异的网络入侵、在线宣传和信息战称为“网络战”、“网络攻击”或“战争行为”。但这些词汇不固定，甚至有歧义。例如，侵入网络以收集信息的活动，可能与传统的情报收集类似，尽管涉及范围有所不同；部分国家在社交媒体上干预选举的活动，则是一种数字化宣传、虚假信息传播和政治干预的结合，其范围和影响远超以往任何时代。数字技术和网络平台的扩展，使依托其上的此类活动成为可能。此外，其他网络行动也可能造成与传统敌对状态类似的实际影响。网络行动的性质、范围及归属的不确定性，可能会使看似基本无误的因素成为各方争论的焦点问题：冲突是否已经开始、与谁冲突、冲突涉及什么、冲突可能升级到何种程度，等等。从这个意义上说，各个大国正陷入一种网络冲突，但这种冲突的性质和范围尚无现成的定义。

我们所处的数字时代面临一个核心悖论：一个社会的数字能力越强，就变得越脆弱。计算机、通信系统、金融市场、大学、医院、航空公司、公共交通系统甚至民主政治都依托于系统，而这些系统或多或少可被网络操纵或攻击。随着发达经济体将数字指挥和控制系统整合到发电厂和电网中、将政府项目转移到大型服务器和云系统中、将数据转誊到电子账簿中，它们在网络攻击面前也就变得倍加脆弱。这些行为提供了更丰富的目标集合，因此仅仅一次成功的攻击就可能造成实质性的破坏。与此相对，面对数字破坏，低技术国家、恐怖组织甚至个人攻击者所承受的损失可能要小得多。

网络能力和网络行动成本较低，且具有相对的可否认性，因此，一些国家可能会使用半自主行为体来执行该功能。与一战前夕遍布巴尔干半岛的准军事集团一样，这些半自主团体可能难以控制，在未经官方批准的情况下开展挑衅活动。网络领域中行为的快速和不可预测性、各种行为体间关的复杂性，以及可以极大削弱一国网络能力并搅乱国内政治格局（即使这些活动不会升级到传统的武装冲突水平）的泄密者和破坏者的存在，都可能诱使决策者“先发制人”，以防遭受致命打击。

网络领域行为的速度和模糊性有利于进攻方，并鼓励以“积极防御”和“向前防御”寻求扰乱、排除攻击。网络威慑可能达到的程度，部分取决于防御者的目标及衡量成功的标准。最有效的攻击往往不会达到武装冲突的传统定义门槛（通常未得到立即承认或正式承认）。即便是为了威慑，也没有任何一个主要的网络行为体公开其全部的能力或活动，无论该行为体是政府的抑或非政府的。因此，尽管新的能力正在涌现，相关战略和理论仍处于隐蔽的阴影中，以不确定的方式演变。我们正处于全新的战略前沿，需要进行系统性的探索，促进政府和工业界密切合作以确保有竞争力的安全能力，还需要大国之间在适当的保障措施下及时就网络军备限制进行讨论。

**人工智能与安全领域的动荡**

核武器的破坏性和网络武器的神秘性，正日益与一种更新的能力相结合，也就是前几章中所谓基于人工智能原理的能力。各国正在悄无声息地、有时是试探性地、但又确凿无疑地发展和部署人工智能，这将促进各种军事能力的战略行动，进而对安全政策产生革命性影响。

将非人类逻辑引入军事系统和过程，会给战略带来改变。通过与人工智能共同培训或与之合作，军队和安全部门的洞察力和影响力显著提升，这既令人惊讶，也令人不安。军队与人工智能的伙伴关系，或将否定传统战略和战术的某些方面，而加强另一些方面。如果人工智能被赋予权限，能够在一定程度上控制（攻击性或防御性的）网络武器或飞机等物理武器，它可能会迅速执行人类难以执行的功能。例如，美国空军的人工智能ARTUμ已经在飞行测试中成功驾驶飞机并操作雷达系统。ARTUμ的研发初衷是在无人干预的情况下做出“最后抉择”，其能力仅限于驾驶飞机和操作雷达系统，但其他国家和设计团队可能就没有那么大的限制了。

人工智能的自主和独立逻辑能力不仅有可能推动变革，还具有一定的不可预料性。大多数传统的军事战略和战术都基于对人类对手的假设，即假设对手的行为和决策计算符合某种可识别的框架，或者可由经验和传统智慧界定。然而，当人工智能驾驶飞机或用雷达扫描目标，它遵循的是其自身的逻辑，难以被人类所理解，也不受传统信号和佯攻的影响，而且，其逻辑执行速度往往高于人类的思维速度。

战争一向充满不确定性和偶然性，但人工智能的进入将为其带来新的变数。人工智能是不断发展变化的新事物，因此，即使是那些创造出人工智能并使其设计或操作的武器的强国，可能也无法确定其威力、无法预判其行动。人工智能可以感知到人类无法或无法快速感知的环境，甚至能以超越人类的思维速度与思维广度进行学习和改进，对于这样一种事物，我们怎么可能制定有针对性的进攻或防御战略呢？如果人工智能辅助武器的效果取决于人工智能在战斗中的感知及从中得出的结论，那么，某些武器的战略效果是否只能在使用中得到证明？如果竞争对手在静默和保密的情况下训练其人工智能，那么在冲突尚未发生时，领导人能知道己方在军备竞赛中是领先还是落后吗？

在传统的冲突中，对手的心理是战略行动瞄准的关键点。但算法只知道指令和目标，而不知士气或怀疑为何物。由于人工智能可以适应其所遇现象，当两个人工智能武器系统彼此对抗时，交战双方都无法准确理解这一交互将产生何种结果、造成哪些附带影响，也就无法清晰地把握对方的能力、预测冲突的代价。对人工智能武器的工程师和构建者而言，由于受到这些限制，他们在研发和制造过程中会更注重提高武器的速度、影响广度和持久度，从而使冲突更激烈、广泛、不可预测。

同时，即使有人工智能辅助，强大的防御也是安全的先决条件。新技术的普遍性使人们无法单方面放弃它。然而，即便是在整兵备战时，各国政府也应该评估并尝试将人工智能逻辑加入人类战斗经验，以使战争变得更人道、更精确。新技术对外交和世界秩序的影响也需加以反思。

人工智能和机器学习可以扩大现有武器的打击能力，进而改变行为者的战略和战术选择。人工智能不仅能提高常规武器的瞄准精度，还能改变瞄准方式，比如（至少在理论上）瞄准某个特定的个人或物体，而非某个地点。通过研究大量信息，人工智能网络武器可以学习如何渗透防御，无需人类寻找可利用的软件漏洞。同样，人工智能也可用于防御，在漏洞被利用之前发现并修复它们。但是，由于攻击者可以选择目标而防御者不能，在人工智能的辅助下，进攻方即便不是战无不胜，也能占得先机。

**如果一个国家面对的对手已经训练人工智能来驾驶飞机、独立瞄准并决定开火，这将如何影响战术、战略或诉诸升级战争规模（甚至是核战）的意愿？**

**人工智能开辟了信息空间能力，包括虚假信息领域。生成式人工智能可以创造大量似是而非的虚假信息，包括伪造的人物、图片、视频和演讲等，以此为信息战和心理战推波助澜。理论上，人工智能可以合成看似真实的冲突照片和视频，让公众人物“发表”他们其实从未说过的言论，并将其以最有效地方式广泛转发给目标群体，迎合人们的偏见和期望。如果一个国家领导人的合成形象被对手操纵，以制造不和或发布误导性指令，公众（甚至其他政府和官员）能够及时识破骗局吗？**

与核武器领域不同的是，对人工智能的使用并不存在公认的禁令，也没有明确的威慑（或升级程度）概念。美国的竞争对手正在打造人工智能辅助武器，包括实体武器和网络武器，据报道，其中一些已投入使用。人工智能大国有能力部署机器和系统，这些系统具有快速逻辑推理和不断演化的行为能力，可用于攻击、防御、监视、传播虚假信息，以及识别和破坏敌方的人工智能。

随着变革性人工智能能力的不断发展和传播，在没有可验证的限制因素的情况下，世界主要国家会继续追求优势地位。它们会假定，新的、可用的人工智能一旦出现，就必定会扩散。由于具备军民双重用途、易于复制和传播，人工智能的基本原理和关键创新在很大程度上是公开的。即使对其加以监管，监管机制也很难做到无懈可击：监管方式可能随着技术进步而过时，也可能被窃取人工智能者发现漏洞。人工智能的新用户可能会调整基础算法，以实现迥然不同的目标，一个社会的商业创新可能会被另一个社会用于维护安全或信息战。政府时常会采纳尖端人工智能发展中最具战略意义的方面，以满足其国家利益设想。

平衡网络力量平衡、将人工智能威慑概念化的相关工作尚处于起步阶段。在对这些概念给出确切定义之前，对此类问题的规划是抽象的。比如，在冲突中，交战一方可能试图使用或威胁使用一种效果未知的武器，以此摧垮对方的意志。

而最具颠覆性且不可预测的影响，可能发生在人工智能和人类智能遭遇之时。纵观历史，积极备战的国家对其对手的理论、战术和战略心理，即便不是洞若观火，至少也有大致了解。这使对抗性战略和战术得以发展，并形成了一系列象征性话语（例如拦截靠近边境的飞机、通过争议水域等）。然而，当由人工智能来制订计划或锁定目标，甚至由人工智能在常规巡逻或冲突期间提供动态协助时，这些原本熟悉的概念和互动可能会变得陌生——人们需要与一种新型智能打交道，而该智能的运作方式和战术都是未知的。

从根本上说，向人工智能和人工智能辅助武器及防御系统的转变，将导致对智能的依赖。这种智能的运作所基于的经验范式与人类有本质区别，且具备可观的分析潜力。在极端情况下，这种依赖甚至演变为一种授权，可能导致未知的风险。因此，人类操作者必须对具有潜在致命影响的人工智能加以监控，即便不能避免所有错误，至少要保障道德责任和问责能力。

**然而，最深层的挑战可能是哲学层面上的。如果战略的分析运作无法再为人类理性所理解，则其过程、范围和最终意义将变得不再透明。如果决策者认为，在揭示现实的最深层模式、了解（可能拥有自己的人工智能的）对手的能力和意图并及时做出应对的过程中，人工智能的辅助已必不可少，那么，将关键决策权下放给机器可能成为必然趋势。各个社会可能会就“哪些决策权可以下放”、“何种风险和后果是可以接受的”等问题，给出各不相同的答案。主要大国应当未雨绸缪，提前就该演变的战略、学说和道德影响展开对话，否则将导致不可逆的影响。国际社会必须做出努力，以限制这些风险。**

**管控人工智能**

在智能体系互相对抗之前，我们必须对这些问题加以考量和理解。随着网络和人工智能能力被用于战略目的，战略竞赛的领域变得更为广阔，从而使这些问题变得迫在眉睫。某种程度上说，网络和人工智能使一切与数字网络相连的地方都成为“战场”。如今，数字程序控制着一个由众多实体系统构成的领域，该领域极为庞大，且仍在增长（某些情况下，甚至连门锁和冰箱都接入了网络）。这催生出一个极其复杂、广泛和脆弱的系统。

对人工智能强国来说，追求某种形式的彼此理解和相互制约至关重要。由于相应系统和能力可以通过计算机代码的变化而轻易地悄然改变，各大政府可能倾向于认为，其对手在战略敏感的人工智能研究、开发和部署方面的步调会比其公开承认甚至私下承诺的更进一步。从纯技术角度来看，让人工智能参与侦察、锁定目标或是展开致命性自主行动并不难，因此，一套相互制约和验证的体系的体系的构建显得既紧迫又困难。

要寻求保障和制约，就必然与人工智能的动态本质相抗衡。由人工智能驱动的网络武器一旦问世，就可能在适应能力和学习能力上远超预期；武器的能力或将随之改变。如果武器能够以某种方式改变，且改变的范围或性质不同于预期，那么威慑和升级的设想可能变得更加扑朔迷离。因此，无论是在初始设计抑或最终部署阶段，都需要调整人工智能的运行范围，以便人类对系统加以监视，在其偏离初始目标时将其关闭或重新定向。为了避免意外、潜在的灾难性后果，这种限制必须是相互的。

无论是限制人工智能和网络能力，抑或遏制其扩散，都非常困难。主要大国开发和使用人工智能和网络的能力，有可能落入恐怖分子和流氓帮派手中。同样，那些没有核武器、常规武器军力也有限的小国，也可以通过投资尖端的人工智能和网络武器，发挥出巨大的影响力。

各国势必会把非连续、非致命的任务委托给人工智能算法（部分由私营实体操作），其中包括执行检测和防止网络空间入侵的防御功能。一个高度网络化和数字化的社会“受攻击面”太大，人类操作者无法仅凭手动实现防御。随着人类生活的许多方面转移到网上，经济也持续数字化，一个流氓网络人工智能就可能破坏整个行业。国家、公司甚至个人都应该着手构建某种失效保护体系，以防患于未然。

这种保护的最极端形式即切断网络连接。对国家而言，离线可能是终极的防御形式。如果排除这种极端措施，那么，能够执行某些重要网络防御功能的就只有人工智能了——网络空间浩瀚无垠，在此可采取的行动选项几乎无穷无尽，因此，除了少数几个国家，这一领域内最重要的防御能力可能是其他国家力所不及的。

除了人工智能防御系统，还有一类最令人头疼的武器——致命自主武器系统。该系统一旦被激活，就可以在没有人类干预的情况下选择目标并实施打击。这类武器的关键问题是人类对其缺乏监督和干预的能力。

一个自主系统“在其指令环路中”的某些行动需要人类授权，或需要一个人“在指令环路上”被动地监控其活动。除非受到可遵守和可证实的相互协议的限制，前者最终可能涵盖全部战略和目标（例如保卫边境或实现针对敌人的特定结果），且无需耗费大量人力。在这些领域，必须确保人类的判断能发挥作用，实现对武器的监督和指导。如果只有一个或少数几个国家接受这些限制，意义是有限的。先进国家的政府应探讨如何以可行的方式进行检查，进而在此前提下实现相互制约。

人工智能的引入，使人们可能为抢占先机而将某种武器仓促投入使用，进而引发冲突。当一个国家担心其对手正在发展自动化军力时，可能会“先发制人”；而如果攻击“成功”了，担忧的合理性也就无法再得到证实或证伪。为防止冲突意外升级，大国应在一个可验证的限制框架内进行竞争。谈判不只包括缓和军备竞赛，还应该确保双方都大致了解对方动向。但双方都必须对一点有所预期（并据此进行筹划）：对方会对自己最具敏感性的秘密有所保留。正如冷战期间的核武器谈判所表明的，国家间永远不会有完全的信任，但这并不意味着无法达成某种程度的谅解。

我们提出这些问题，是为了界定人工智能给战略带来的挑战。界定核时代的条约（以及随之而来的沟通、执行和核查机制）给我们带来了各方面的收益，但并非历史的必然产物，而是人类能动性的产物，是共同承担危机和责任的产物。

**对民用和军事技术的影响**

传统上，三个技术特性促成了军事和民用领域的分野：技术差异、集中控制和影响规模。所谓技术差异，是指军用和民用技术的区别。所谓集中控制，是指部分技术易于被政府管控，与容易传播、规避政府控制的技术相反。所谓影响规模，则是指一项技术的破坏性潜力。

纵观历史，许多技术都是军民两用的。至于其他技术，一些很容易广泛传播，另一些则具有巨大的破坏力。然而，迄今为止，还没有一种技术同时具备以下三种特性：军民两用、易于传播和潜在的巨大破坏性。运送货物到市场的铁路和运送士兵到战场的铁路是一样的，铁路不具有破坏性潜力。核技术通常是军民两用的，且破坏性巨大，但核设施很复杂，这使政府能够相对安全地控制核技术。猎枪可能被广泛使用，同时具有军民用途，但其有限的能力使持枪者无法在战略层面造成破坏。

而人工智能打破了这种范式。很明显，人工智能可以军民两用；也很容易传播，只需几行代码，大多数算法（除了部分例外）可以在单个计算机或小型网络上运行，这意味着政府很难通过控制基础设施来控制这种技术；其应用具有巨大的破坏性潜力。这种独一无二的特性组合，加上广泛的利益相关者，产生了具有全新复杂性的战略挑战。

人工智能赋能武器，使对手能以惊人的速度发起数字攻击，并极大提高其利用数字漏洞的能力。这样一来，对于即将到来的攻击，一个国家可能还来不及评估就需立即响应，否则就可能被对方解除武装。如果一个国家有相应手段，就可以在对方完全展开攻击之前做出回应，构建一个人工智能系统来预警攻击并加以反击。这一系统的存在，及其毫无预警地实施行动的能力，可能会刺激另一方投入更多建设和规划，包括开发并行技术或基于不同算法的技术。如果人类也参与了这些决定，那么除非各方谨慎地发展出一个共同的限制理念，否则先发制人的冲动可能会压倒谋定后动的需要，就像20世纪初的情况一样。

在股票市场，一些复杂的所谓量化公司认识到，人工智能算法可以发现市场模式，并比最佳操盘手更快地做出反应。因此，这些公司已将其证券交易的部分控制权委托给算法。算法系统所赢得的利润通常远超人类操盘手。然而，它们偶尔会严重误判，误判程度远超最糟糕的人为错误。

在金融领域，此类错误会毁掉投资，但不会致人死地。然而，在战略领域，一次类似“闪电崩盘”的算法故障可能引发灾难性后果。如果数字领域的战略防御需要战术上的进攻，那么当一方在此类计算或行动上出错时，就可能在不经意间使冲突升级。

将这些新能力纳入一个明确的战略和国际均势概念的尝试非常复杂，因为技术优势所需的专业知识不再完全集中于政府方面。从传统的政府承包商到个人发明家、企业家、初创企业和私人研究实验室，各种各样的行为体和机构都参与到对这一具有战略意义的技术的塑造过程中，而并非其中所有人都认为其使命应与联邦政府所界定的国家目标保持内在一致。工业界、学术界和政府之间的相互教育过程可以帮助弥合这一鸿沟，并确保各方在一个共同的概念框架内理解人工智能战略意义的关键原则。很少有哪个时代面临过如此局面：一方面，其遭遇的战略和技术挑战如此复杂；另一方面，对该挑战的性质乃至讨论其所需的词汇却鲜有共识。

核时代尚未解决的挑战是：人类发展了一种技术，战略家们却找不到可行的军事行动理论。人工智能时代的困境与之不同：典型技术将被广泛获取、掌握和应用。无论是在理论概念上还是在实践操作中，实现相互间的战略克制，甚至是实现对“克制”的共同定义，都将空前地困难。

即使历经半个世纪的努力，如今对核武器的管控仍不尽完善。然而，评估核平衡其实相对简单：核弹头可以计数，其生产也是已知的。人工智能则不同：其能力不是固定的，而是动态变化的。与核武器不同的是，人工智能很难被追查：一旦经过训练，它们可以被轻易复制，并在相对较小的机器上运行。以目前的技术，对其存在进行证实或证伪，将是极其困难甚至无法实现的。在这个时代，威慑可能来自一种复杂性——来自人工智能攻击能够借助载体的多样性，也来自潜在的人工智能反应速度。

为了管控人工智能，战略家必须考虑如何将其纳入负责任的国际关系模式。在部署武器之前，战略家必须了解使用武器的迭代效应、这些武器导致冲突升级的可能性和谋求冲突降级的途径。负责任地使用策略，再辅以制约原则，将是必不可少的举措。决策者应致力于同时处理军备、防御技术和战略，以及军备控制问题，而不是将其视为在时间上前后不同、在功能上彼此对立的步骤。必须在技术付诸使用前就制定理论并做出决定。

那么，这种制约的要求是什么呢？一个显见的出发点就是以传统强制方式对能力加以制约。在冷战期间，这种做法获得了一些进展，至少在象征意义上如此。一些能力受到了限制（如弹头），另一些（如中程导弹）则被彻底禁止。但无论是限制人工智能的潜在能力，还是限制人工智能的数量，都不能完全符合这种技术在民用领域的广泛应用和持续发展态势。我们必须研究新的限制因素，重点是人工智能的学习和目标锁定能力。

在一项部分预见到这一挑战的决定中，美国对“人工智能赋能武器”和“人工智能武器”进行了划分，前者使人类指挥的战争更精确、更致命、更有效，后者则能脱离人类操作者自主做出致命的决定。美国已宣布其目标是将人工智能使用限制在前一种类别中，并谋求建立一个任何国家（包括美国自身）都不拥有后一种武器的世界。这种划分称得上明智。与此同时，技术的学习和演进能力也可能导致对某些特定能力的限制不足。对人工智能赋能武器的制约性质和制约方式进行界定，并确保约束是相互的，是关键所在。

在19世纪和20世纪，各国逐渐对某些形式的战争进行了限制（例如使用化学武器、攻击平民）。鉴于人工智能武器使大量新类别的军事活动成为可能，或使旧形式的军事活动重获新生，世界各国必须及时界定，何种军事行为有可能背离人性尊严和道德责任。要获得安全，我们就不能只是被动应对，而要未雨绸缪。

与人工智能相关的武器技术带来了这样的困境：对国家而言，技术的持续研发至关重要，否则我们将失去商业竞争力和与世界的关联性；但新技术所固有的扩散天性使迄今一切以谈判促限制的努力付诸东流，甚至连概念也未能形成。

**新世界中的古老追求**

各个主要的技术先进国家都需要明白，它们正处于战略转型的门槛上，这种转型与当年核武器的出现同等重要，但影响将更加多样化、分散化和不可预测。如果一个社会致力于扩展人工智能前沿，就应当成立一个国家层面的机构，来考量人工智能的防御和安全，并在各个相关部门之间建立桥梁。这个机构应被赋予两项职能：维持本国的竞争力，同时协调研究如何防止或至少限制不必要的冲突升级或危机。在此基础上，与盟友和对手进行某种形式的谈判将是至关重要的。

如果要对这一方向进行探索，那么世界两大人工智能强国——美国和中国——就必须接受这一现实。两国可能会得出这样的结论：无论两国新一阶段的竞争将以何种形式展开，都应就“不打前沿技术战争”形成共识。双方政府可以委托某个团队或高级官员负责监督，并直接向领导人报告潜在的危险及规避方式。截至本书撰写时，这种努力与两国的公众情绪并不相符。然而，这两个大国互相对峙而拒不进行对话的时间越长，发生意外的可能性就越大。一旦发生意外，双方都会被其技术和部署预案所驱使，陷入双方都不愿意看到的危机，甚至可能引发全球规模的军事冲突。

国际体系的矛盾之处在于，每个大国都被驱使采取行动，也必须采取行动，从而最大限度地保障自身的安全；然而，为了避免危机接踵而来，每个国家都必须对维持普遍和平有一定的责任感。这个过程涉及对限制的认识。军事规划人员或安全官员会根据可能发生的最坏情况考虑问题（这样做并没错），并优先寻求获取应对这些情况所需的能力。政治家（可能也就是上述这批人）则有义务考虑如何使用这些能力，以及使用之后的世界将会是什么样子。

在人工智能时代，我们应该对长期以来的战略逻辑进行调整。在灾难真正发生之前，我们需要克服，或者至少是遏制这种自动化的驱向。我们必须防止运行速度比人类决策者更快的人工智能做出一些具有战略后果的、不可挽回的行为。防御力量自动化的前提是不放弃人类控制。该领域固有的模糊性，再加上人工智能的动态性、突出性及其传播的便利性，将使评估复杂化。在此前的时代，只有少数几个大国或超级大国有责任限制自己的破坏性能力，以避免发生灾难。但不久后，随着人工智能技术的扩散，更多的行为体也必须承担类似的使命。

当代领导人可以通过将常规能力、核能力、网络能力和人工智能能力广泛而动态地结合在一起，来实现控制武器装备的六大任务。

第一，相互对抗或敌对的国家的领导人必须定期对话，讨论双方都想避免的战争形式，就像美苏在冷战期间所做的那样。为此，美国及其盟友应该围绕其共同的、固有的、不可侵犯的利益与价值观组织起来，这些利益和价值观包括在冷战结束后几代人的经验。

必须对核战略的未解难题给予新的关注，并认识到其本质正是人类在战略、技术和道德方面遭遇的巨大挑战之一。几十年来，对广岛和长崎被核弹化为焦土的记忆迫使人们认识到核问题的不寻常性和严峻程度。正如美国前国务卿乔治·舒尔茨在2018年对国会所说：“我担心人们已经失去了那种恐惧感。”拥核国家的领导人必须认识到，他们有责任共同努力，以防止灾难的发生。

网络与人工智能技术的领先大国应该努力界定其理论和限制（即使其所有方面均未被公开），并找出自身理论与竞争大国之间的关联点。如果我们的意图是威慑而非使用，是和平而非冲突，是有限冲突而非普遍冲突，就需要以反映网络和人工智能独特蕴含的措辞来重新理解和界定这些术语。

拥核国家应承诺对其指挥控制系统和早期预警系统进行内部检查。这类失效保护检查应确定检查步骤，以加强对网络威胁和在未经授权、疏忽或意外情况下使用大规模毁灭性武器行为的防范。这些检查还应包括选项，以排除对核指挥控制系统或早期预警系统相关设施的网络攻击。

世界各国，特别是技术强国，应制定强有力和可接受的方法，在紧张局势加剧和极端情况下尽量延长决策时间。这应该成为竞争对手之间共同的概念性目标，将控制不稳定性和建立共同安全所需的步骤（既有当前的，也有长期的）联系起来。在危机中，人类必须对是否使用先进武器承担最终责任。竞争对手尤其应该努力就一种机制达成一致，以确保那些或许不可撤销的决定是有利于人类生存的。

主要的人工智能大国应该考虑如何限制军事化人工智能的继续扩散，或者依靠外交手段与武力威胁，开展系统性的防扩散工作。那些野心勃勃地想要将技术用于不可接受的破坏性目的的技术收购者会是谁？有哪些特定的人工智能武器值得我们特别关注？谁来确保这条红线不被逾越？老牌的核大国探索过这种防核扩散概念，结果成败参半。如果一种具有颠覆性和潜在破坏性的新技术被用于武装世界上怀有最强烈敌意或道德上最不受约束政府的军队，那么战略均势可能难以实现，冲突也可能无法控制。

由于大多数人工智能技术具有军民两用特性，我们有责任在这场技术研发竞赛中保持领先。但这也同样迫使我们去理解人工智能技术的局限性。等到危机来临才开始讨论这些问题就为时已晚了。一旦在军事冲突中使用，人工智能技术的响应速度之快，几乎注定它将以比外交手段更快的速度产生结果。大国之间必须就网络和人工智能武器展开讨论，哪怕只是为了形成一套共同的战略概念话语，以及对彼此红线的感知。要在最具破坏性的能力上实现相互制约，绝不能等到悲剧发生再去亡羊补牢。当人类开始在创造新的、不断演化的、拥有智能的武器方面展开竞争时，设限的失败不会被历史原谅。在人工智能时代，对国家优势的持久追求，仍须以捍卫人类伦理为前提。

&#x20;   进入专题： [世界秩序](https://www.aisixiang.com/data/search?searchfield=keywords\&keywords=%e4%b8%96%e7%95%8c%e7%a7%a9%e5%ba%8f) &#x20;

本文责编：SuperAdmin\
发信站：爱思想（https://www.aisixiang.com）\
栏目： [学术](https://www.aisixiang.com/data/search?column=207) > [国际关系](https://www.aisixiang.com/academic/guojiguanxi.html) > [大国关系与国际格局](https://www.aisixiang.com/data/search?column=606)\
本文链接：https://www.aisixiang.com/data/143416.html\
文章来源：本文转自观察者基辛格，转载请注明原始出处，并遵守该处的版权规定。
